{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abc Def Ghi\n",
      "ABC DEF GHI\n",
      "abc def ghi\n"
     ]
    }
   ],
   "source": [
    "s1 = \"abc def ghi\"         \n",
    "print(s1.title())       #将字符串s1中每个单词首字母转换为大写\n",
    "print(s1.upper())       #将字符串s1中每个字母改为全部大写\n",
    "s2 = \"ABC DEF GHI\"\n",
    "print(s2.lower())        #将字符串s2中每个单词改为全部小写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"Hello world!\")                 #计算字符串\"Hello world!\"所包含的字符数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love the world!\n",
      "  I love the world!\n",
      "I love the world!  \n"
     ]
    }
   ],
   "source": [
    "s = \"  I love the world!  \"           #创建一个新的字符串并命名为s\n",
    "print(s.strip())                      #删除字符串s开头和末尾处的空白\n",
    "print(s.rstrip())                     #删除字符串s末尾处的空白\n",
    "print(s.lstrip())                     #删除字符串s开头处的空白"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jonny,Nice', 'to', 'meet', 'you!']\n",
      "['Jonny', 'Nice to meet you!']\n",
      "['Jonny,Nice', 'to', 'meet', 'you!']\n"
     ]
    }
   ],
   "source": [
    "s = \"Jonny,Nice to meet you!\"      #创建一个新的字符串并命名为s\n",
    "print(s.split(' '))                #以空格作为分隔符拆分字符串s\n",
    "print(s.split(','))                #以逗号作为分隔符拆分字符串s\n",
    "print(s.split())                   #不指定分隔符拆分字符串s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thwas was an apple, and thwas was really big!\n",
      "thwas was an apple, and thwas is really big!\n"
     ]
    }
   ],
   "source": [
    "s = \"this is an apple, and this is really big!\"\n",
    "print (s.replace(\"is\",\"was\"))         #将字符串s中所有的is都替换成was\n",
    "print (s.replace(\"is\", \"was\", 3))     #将字符串s中前三个is都替换成was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thwas was an apple, and thwas was really big!\n"
     ]
    }
   ],
   "source": [
    "s = \"this is an apple, and this is really big!\"\n",
    "print (s.replace(\"is\", \"was\", 10))   #将字符串se中前十个is都替换成was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be str, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-acbed4d0781a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Amy\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m18\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"is\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mage\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m\" years old.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: must be str, not int"
     ]
    }
   ],
   "source": [
    "name = \"Amy\"\n",
    "age = 18\n",
    "info = name + \"is\" + age +\" years old.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amy is 18 years old.\n"
     ]
    }
   ],
   "source": [
    "name = \"Amy\"\n",
    "age = 18\n",
    "info = name + \" is \" + str(age) +\" years old.\"\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('l', 'n')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LS='literalstring'\n",
    "LS[0],LS[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LS='literalstring'\n",
    "LS.find(\"i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "literalstring\n",
      "teralstring\n",
      "literals\n",
      "terals\n",
      "trl\n",
      "srn\n",
      "teralstring\n"
     ]
    }
   ],
   "source": [
    "LS='literalstring'      #创建一个字符串并命名为LS\n",
    "print(LS[:])             #输出字符串LS\n",
    "print(LS[2:])            #从偏移量2提取到字符串最后（不包含第2个字母）\n",
    "print(LS[:8])            #从字符串开始提取到偏移量8（包含第8个字母）\n",
    "print(LS[2:8])           #从偏移量2（不包含2）提取至偏移量8（包含8）\n",
    "print(LS[2:8:2])         #从偏移量2（不包含2）每2个字母提取至偏移量8（包含8）\n",
    "print(LS[-6:-1:2])      #从倒数第6个（包含倒数第6）每2个提取至倒数第1个（不包倒数第1）\n",
    "print(LS[2:30])       #从偏移量10（不包含10）提取至偏移量30（包含30），索引过大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始字符串: one1two2three3four4\n",
      "懒惰匹配: o.*?e\n",
      "懒惰匹配结果: ['one', 'o2thre']\n",
      "贪婪匹配: o.*e\n",
      "贪婪匹配结果: ['one1two2three']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "str1 = \"one1two2three3four4\"\n",
    "print(\"原始字符串:\",str1)\n",
    "regexL = \"o.*?e\"\n",
    "print(\"懒惰匹配:\",regexL) \n",
    "listL = re.findall(regexL,str1)      # 懒惰匹配\n",
    "print(\"懒惰匹配结果:\",listL)\n",
    "regexT = \"o.*e\"\n",
    "print(\"贪婪匹配:\",regexT)\n",
    "listT = re.findall(regexT,str1)      # 贪婪匹配\n",
    "print(\"贪婪匹配结果:\",listT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 3), match='abc'>\n",
      "<_sre.SRE_Match object; span=(0, 9), match='abcabcabc'>\n",
      "<_sre.SRE_Match object; span=(0, 6), match='abcabc'>\n"
     ]
    }
   ],
   "source": [
    "mystr = 'abcabcabc'\n",
    "print(re.match('(abc)',mystr))\n",
    "print(re.match('(abc)+',mystr))\n",
    "print(re.match('(abc){1,2}',mystr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 3), match='one'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print(re.match('one', 'one two three'))\n",
    "print(re.match('three', 'one two three'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "match=re.match('one', 'one two three')\n",
    "print(match.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "search =re.search('one', 'one two three')\n",
    "print(search.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re.match: None\n",
      "re.seartch: dogs\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "s = \"Cats are smarter than dogs\";\n",
    "match = re.match('dogs',s)\n",
    "if match:\n",
    "   print (\"re.match:\",match.group())\n",
    "else:\n",
    "   print (\"re.match: None\")\n",
    " \n",
    "search = re.search('dogs',s)\n",
    "if search:\n",
    "   print (\"re.seartch:\",search.group())\n",
    "else:\n",
    "   print (\"re.seartch: None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the sum of 7 and 9 is 16.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "s = \"the sum of 7 and 9 is 15.\"\n",
    "print(re.sub('15', '16', s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the sum of 14 and 18is 32.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def double(matched):\n",
    "    value = int(matched.group('value'))# 将匹配的数字乘以2\n",
    "    return str(value*2)\n",
    "s = 'the sum of 7 and 9is 16.'\n",
    "print(re.sub('(?P<value>\\d+)', double, s)) #命名一个名字为value的组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of match pattern is None\n",
      "Result of search pattern is <_sre.SRE_Match object; span=(3, 6), match='two'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = re.compile('two')                # 用于匹配字符串‘two’\n",
    "m = pattern.match('onetwothree123')        # 查找头部，没有匹配\n",
    "n = pattern.search('onetwothree123')       # 全程查找，有匹配\n",
    "print('Result of match pattern is',m)\n",
    "print('Result of search pattern is',n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '4']\n",
      "['1', '2']\n",
      "['1', '2', '3', '4']\n"
     ]
    }
   ],
   "source": [
    "s = 'one1two2three3four4'\n",
    "pattern = re.compile('\\d+')      # 查找数字，使用compile 预编译后使用 findall\n",
    "\n",
    "print(pattern.findall(s))\n",
    "print(pattern.findall(s,0,10))   # 限定进行匹配字符串长度\n",
    "print(re.findall('\\d+', s))      # 不使用 compile 直接使用re.findall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "it = re.finditer('\\d+','one1two2three3four4') \n",
    "for match in it: \n",
    "    print (match.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'two', 'three', 'four', '']\n",
      "['one1two2three3four4']\n",
      "['one', 'two2three3four4']\n"
     ]
    }
   ],
   "source": [
    "s = 'one1two2three3four4'\n",
    "print( re.split('\\d+', s))  # 按照数字切分\n",
    "print(re.split('a', s, 1))  # a匹配不到 返回包含自身的列表\n",
    "print( re.split('\\d+', s, 1)) # maxsplit 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 11), match='Hello wide '>\n",
      "Hello wide \n",
      "(0, 11)\n",
      "('Hello', 'wide')\n",
      "Hello\n",
      "(0, 5)\n",
      "0\n",
      "5\n",
      "wide\n",
      "(6, 10)\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile('([a-z]+) ([a-z]+) ', re.I)   # re.I 表示忽略大小写\n",
    "m = pattern.match('Hello wide wide world')\n",
    "print(m)                             # 匹配成功，返回一个 Match 对象\n",
    "print(m.group())                     # 返回匹配成功的整个子串\n",
    "print(m.span())                      # 返回匹配成功的整个子串的索引\n",
    "print(m.groups())                    # 等价于 (m.group(1), m.group(2), ...) \n",
    "print(m.group(1))                    # 返回第一个分组匹配成功的子串\n",
    "print(m.span(1))                     # 返回第一个分组匹配成功的子串的索引\n",
    "print(m.start(1))                    # 返回第一个分组匹配成功的子串的开始位置\n",
    "print(m.end(1))                      # 返回第一个分组匹配成功的子串的结束位置\n",
    "print(m.group(2))                    # 返回第二个分组匹配成功的子串\n",
    "print(m.span(2))                     # 返回第二个分组匹配成功的子串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re.S使.匹配包括换行在内的所有字符 [('great', ' \\n        object-oriented,\\n        ', 'inter')]\n",
      "re.I不区分大小写: ['And interactive ']\n",
      "re.M多行匹配 ['interpreted, ', 'interactive ']\n"
     ]
    }
   ],
   "source": [
    "s = \"\"\"\n",
    "        Python is a great \n",
    "        object-oriented,\n",
    "        interpreted, \n",
    "        And interactive \n",
    "        programming language.\n",
    "        \"\"\"\n",
    "re1= re.findall(r'(great)(.*?)(inter)',s,re.S)\n",
    "print(\"re.S使.匹配包括换行在内的所有字符\",re1)\n",
    "re2 = re.findall(r'(and.*)',s,re.I)\n",
    "print(\"re.I不区分大小写:\",re2)\n",
    "re3 = re.findall(r'(inter.*)',s,re.M)\n",
    "print(\"re.M多行匹配\",re3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = re.compile(r\"\"\"\n",
    "# 匹配数字或字母\n",
    "/d+\n",
    "# 数字\n",
    "| \n",
    "[a-zA-Z]+\n",
    "# 字母\n",
    "\"\"\", re.X)\n",
    "result = pattern.match('abc')\n",
    "print(result.group())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入用户名（quit退出）：wuhan\n",
      "用户名： wuhan 匹配!\n",
      "请输入用户名（quit退出）：123wh\n",
      "用户名： 123wh 匹配!\n",
      "请输入用户名（quit退出）：wuhan123\n",
      "用户名： wuhan123 匹配!\n",
      "请输入用户名（quit退出）：_123wh\n",
      "用户名： _123wh 不匹配!\n",
      "请输入用户名（quit退出）：wh_345\n",
      "用户名： wh_345 不匹配!\n",
      "请输入用户名（quit退出）：wu$han\n",
      "用户名： wu$han 不匹配!\n",
      "请输入用户名（quit退出）：quit\n",
      "程序完成！\n"
     ]
    }
   ],
   "source": [
    "#用户名匹配\n",
    "import re\n",
    "pattern='^[a-zA-Z0-9]+$'\n",
    "username=input('请输入用户名（quit退出）：')\n",
    "while(username != 'quit'):\n",
    "    result=re.match(pattern,username)\n",
    "    if result:\n",
    "        print('用户名：',username,'匹配!')\n",
    "    else:\n",
    "        print('用户名：',username,'不匹配!')\n",
    "    username=input('请输入用户名（quit退出）：')\n",
    "\n",
    "print('程序完成！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入密码（quit退出）：12345678\n",
      "密码： 12345678 不匹配!\n",
      "密码（quit退出）：12345678abc\n",
      "密码： 12345678abc 不匹配!\n",
      "密码（quit退出）：12345678AA\n",
      "密码： 12345678AA 不匹配!\n",
      "密码（quit退出）：12345678Abc\n",
      "密码： 12345678Abc 匹配!\n",
      "密码（quit退出）：quit\n",
      "程序完成！\n"
     ]
    }
   ],
   "source": [
    "#用户密码匹配\n",
    "import re\n",
    "pattern='^(?=.*[a-z])(?=.*[A-Z])(?=.*\\d)[a-zA-Z\\d]{8,}$'\n",
    "password=input('请输入密码（quit退出）：')\n",
    "while(password != 'quit'):\n",
    "    result=re.match(pattern,password)\n",
    "    if result:\n",
    "        print('密码：',password,'匹配!')\n",
    "    else:\n",
    "        print('密码：',password,'不匹配!')\n",
    "    password=input('密码（quit退出）：')\n",
    "\n",
    "print('程序完成！')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.abc123 不匹配\n",
      "http://www.abc123.cn 匹配\n",
      "http://www.abc123.com 匹配\n"
     ]
    }
   ],
   "source": [
    "#url匹配\n",
    "import re\n",
    "pattern=r'^(http:)/{2}w{3}\\.[a-z0-9A-Z]+\\.(com|cn|net)'\n",
    "url1='http://www.abc123'\n",
    "url2='http://www.abc123.cn'\n",
    "url3='http://www.abc123.com'\n",
    "result1=re.match(pattern,url1)\n",
    "if result1:\n",
    "    print(url1, '匹配')\n",
    "else:\n",
    "    print(url1, '不匹配')\n",
    "result2=re.match(pattern,url2)\n",
    "if result2:\n",
    "    print(url2, '匹配')\n",
    "else:\n",
    "    print(url2, '不匹配')\n",
    "result3=re.match(pattern,url3)\n",
    "if result3:\n",
    "    print(url3, '匹配')\n",
    "else:\n",
    "    print(url3, '不匹配')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whuer123@123.com 匹配\n",
      "小明123@123.com 不匹配\n",
      "whuer123@12 不匹配\n"
     ]
    }
   ],
   "source": [
    "#电子邮箱匹配\n",
    "import re\n",
    "pattern='[a-zA-Z0-9_-]+@+[a-z0-9A-Z]+\\.(com|cn|net)'\n",
    "email1='whuer123@123.com'\n",
    "email2='小明123@123.com'\n",
    "email3='whuer123@12'\n",
    "\n",
    "result=re.match(pattern,email1)\n",
    "if result:\n",
    "    print(email1, '匹配')\n",
    "else:\n",
    "    print(email1, '不匹配')\n",
    "\n",
    "result=re.match(pattern,email2)\n",
    "if result:\n",
    "    print(email2, '匹配')\n",
    "else:\n",
    "    print(email2, '不匹配')\n",
    "result=re.match(pattern,email3)\n",
    "if result:\n",
    "    print(email3, '匹配')\n",
    "else:\n",
    "    print(email3, '不匹配')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Brett\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.931 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我/是/一名/大学生/，/我/喜欢/自然语言/处理/。\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "seg_list = jieba.cut(\"我是一名大学生，我喜欢自然语言处理。\") \n",
    "print(\"/\".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'a', 'college', 'student', '.', 'I', 'love', 'natural', 'language', 'processing', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"I am a college student. I love natural language processing.\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 r\n",
      "是 v\n",
      "一名 m\n",
      "大学生 n\n",
      "， x\n",
      "我 r\n",
      "喜欢 v\n",
      "自然语言 l\n",
      "处理 v\n",
      "。 x\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "words = pseg.cut(\"我是一名大学生，我喜欢自然语言处理。\") \n",
    "for word, flag in words:\n",
    "    print('%s %s' % (word, flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('depository_financial_institution.n.01')\n",
      "a financial institution that accepts deposits and channels the money into lending activities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "took 9.732271909713745 secs.\n"
     ]
    }
   ],
   "source": [
    "from pywsd.lesk import simple_lesk\n",
    "sent = 'I went to the bank to deposit my money.'\n",
    "ambiguous = 'bank'\n",
    "answer = simple_lesk(sent, ambiguous, pos='n')\n",
    "print(answer)\n",
    "print(answer.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Adam, how are you?', 'I hope everything is going well.', 'Today is a good day, see you dude.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"Hello Adam, how are you? I hope everything is going well. Today is a good day, see you dude.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Adam', ',', 'how', 'are', 'you', '?', 'I', 'hope', 'everything', 'is', 'going', 'well', '.', 'Today', 'is', 'a', 'good', 'day', ',', 'see', 'you', 'dude', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Hello Adam, how are you? I hope everything is going well. Today is a good day, see you dude.\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "print(porter_stemmer.stem('working'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('works'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('playing',pos=\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'does', 'the', 'fox', 'say']\n",
      "[('what', 'WDT'), ('does', 'VBZ'), ('the', 'DT'), ('fox', 'NNS'), ('say', 'VBP')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text=nltk.word_tokenize('what does the fox say')\n",
    "print(text)\n",
    "print(nltk.pos_tag(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\Brett\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.253 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "默认模式:  我/是/一名/武汉大学/的/学生\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "seg_list = jieba.cut(\"我是一名武汉大学的学生\")  # 使用默认模式，默认是精确模式\n",
    "print(\"默认模式: \",\"/\".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我校 0.3650310687908397\n",
      "自强 0.36010570737862596\n",
      "弘毅 0.3397889749526718\n",
      "校训 0.2773034698328244\n",
      "拓新 0.18251553439541984\n",
      "语出 0.17087980841068703\n",
      "求是 0.16727081943206107\n",
      "自强不息 0.16308094392519082\n",
      "武汉大学 0.14590644626137403\n",
      "中华民族 0.12336785834534353\n",
      "含义 0.11406043013648855\n",
      "伟大 0.1062369610119084\n",
      "不断进取 0.10082084329312978\n",
      "明诚 0.09772568979618321\n",
      "天行健 0.09552964344198472\n",
      "奋发向上 0.09382625755343511\n",
      "修学 0.09382625755343511\n",
      "好古 0.09382625755343511\n",
      "传统美德 0.0924344899442748\n",
      "校风 0.0924344899442748\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "sentence = '1993年，在广泛征求各方面意见的基础上，经校务委员会审议，武汉大学新校训定为：自强 弘毅 求是 拓新。“自强”语出《周易》“天行健、君子以自强不息”。意为自尊自重，不断自力图强，奋发向上。自强是中华民族的传统美德，成就事业当以此为训。我校最早前身为“自强学堂”，其名也取此意。“弘毅”出自《论语》“士不可以不弘毅，任重而道远”一语。意谓抱负远大，坚强刚毅。我校30年代校训“明诚弘毅”就含此一词。用“自强”、“弘毅”，既概括了上述含义，又体现了我校的历史纵深与校风延续。“求是”即为博学求知，努力探索规律，追求真理。语出《汉书》“修学好古，实事求是”。“拓新”，意为开拓、创新，不断进取。概言之，我校新校训的整体含义是： 继承和发扬中华民族自强不息的伟大精神，树立为国家的繁荣昌盛刻苦学习、积极奉献的伟大志向，以坚毅刚强的品格和科学严谨的治学态度，努力探求事物发展的客观规律，开创新局面，取得新成绩，办好社会主义的武汉大学，不断为国家作出新贡献。'\n",
    "\n",
    "keywords=jieba.analyse.extract_tags(sentence,topK=20,withWeight=True,allowPOS=())\n",
    "for item in keywords:\n",
    "    print(item[0],item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "国家 1.0\n",
      "含义 0.9514975766301668\n",
      "校训 0.8831816220202372\n",
      "创新 0.7692786654850344\n",
      "客观规律 0.6586080643543306\n",
      "探求 0.6580889640584097\n",
      "历史 0.5969396515025892\n",
      "自强 0.591162600642535\n",
      "成绩 0.578783632054623\n",
      "取得 0.5695284955354087\n",
      "求知 0.5629593666958073\n",
      "探索 0.5608546033210623\n",
      "态度 0.5597483134160578\n",
      "奉献 0.5595424204402842\n",
      "局面 0.5572237414652758\n",
      "事物 0.5203076902458431\n",
      "校务 0.5201438592373493\n",
      "方面 0.5199791277209296\n",
      "审议 0.5190906744439646\n",
      "意见 0.5181603419662032\n"
     ]
    }
   ],
   "source": [
    "keywords = jieba.analyse.textrank(sentence, topK=20, withWeight=True, allowPOS=('ns','n','vn','v'))\n",
    "for item in keywords:\n",
    "    print(item[0],item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我喜欢自然语言处理。\n",
      "我也是！\n"
     ]
    }
   ],
   "source": [
    "from pyltp import SentenceSplitter\n",
    "sents = SentenceSplitter.split('我喜欢自然语言处理。我也是！')\n",
    "print ('\\n'.join(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我\t喜欢\t自然\t语言\t处理\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "LTP_DATA_DIR = 'D:\\LTP\\ltp_data'  # ltp模型目录的路径\n",
    "cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')  #分词模型路径，模型名称为`cws.model`\n",
    "from pyltp import Segmentor\n",
    "segmentor = Segmentor()  # 初始化实例\n",
    "segmentor.load(cws_model_path)  # 加载模型\n",
    "words = segmentor.segment('我喜欢自然语言处理')  # 分词\n",
    "print ('\\t'.join(words))\n",
    "segmentor.release()  # 释放模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r\tv\tn\tn\tv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "LTP_DATA_DIR = 'D:\\LTP\\ltp_data'      # ltp模型目录的路径\n",
    "pos_model_path = os.path.join(LTP_DATA_DIR, 'pos.model')  \n",
    "# 词性标注模型路径，模型名称为`pos.model`\n",
    "from pyltp import Postagger\n",
    "postagger = Postagger() # 初始化实例\n",
    "postagger.load(pos_model_path)  # 加载模型\n",
    "words = ['我', '喜欢', '自然', '语言','处理']  # 分词结果\n",
    "postags = postagger.postag(words)  # 词性标注\n",
    "print ('\\t'.join(postags))\n",
    "postagger.release()  # 释放模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\tO\tO\tO\tO\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "LTP_DATA_DIR = 'D:\\LTP\\ltp_data'  # ltp模型目录的路径\n",
    "ner_model_path = os.path.join(LTP_DATA_DIR, 'ner.model')  # 命名实体识别模型路径，模型名称为`pos.model`\n",
    "from pyltp import NamedEntityRecognizer\n",
    "recognizer = NamedEntityRecognizer() # 初始化实例\n",
    "recognizer.load(ner_model_path)  # 加载模型\n",
    "words = ['我', '喜欢', '自然', '语言','处理']\n",
    "postags = ['r', 'v','n','n', 'v']\n",
    "netags = recognizer.recognize(words, postags)  # 命名实体识别\n",
    "print ('\\t'.join(netags))\n",
    "recognizer.release()  # 释放模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2:SBV\t0:HED\t4:ATT\t5:ATT\t2:VOB\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "LTP_DATA_DIR = 'D:\\LTP\\ltp_data'  # ltp模型目录的路径\n",
    "par_model_path = os.path.join(LTP_DATA_DIR, 'parser.model')  # 依存句法分析模型路径，模型名称为`parser.model`\n",
    "from pyltp import Parser\n",
    "parser = Parser() # 初始化实例\n",
    "parser.load(par_model_path)  # 加载模型\n",
    "words = ['我', '喜欢', '自然', '语言','处理']\n",
    "postags = ['r', 'v','n','n', 'v']\n",
    "arcs = parser.parse(words, postags)  # 句法分析\n",
    "print (\"\\t\".join(\"%d:%s\" % (arc.head, arc.relation) for arc in arcs))\n",
    "parser.release()  # 释放模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 A0:(0,0)A1:(2,4)\n",
      "4 A1:(2,3)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "LTP_DATA_DIR = 'D:\\LTP\\ltp_data'  # ltp模型目录的路径\n",
    "srl_model_path = os.path.join(LTP_DATA_DIR, 'pisrl_win.model')  # 语义角色标注模型目录路径，模型目录为`srl`。\n",
    "from pyltp import SementicRoleLabeller\n",
    "labeller = SementicRoleLabeller() # 初始化实例\n",
    "labeller.load(srl_model_path)  # 加载模型\n",
    "words = ['我', '喜欢', '自然', '语言','处理']\n",
    "postags = ['r', 'v','n','n', 'v']\n",
    "# arcs 使用依存句法分析的结果\n",
    "roles = labeller.label(words, postags, arcs)  # 语义角色标注\n",
    "# 打印结果\n",
    "for role in roles:\n",
    "    print (role.index, \"\".join(\n",
    "        [\"%s:(%d,%d)\" % (arg.name, arg.range.start, arg.range.end) for arg in role.arguments]))\n",
    "labeller.release()  # 释放模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a college student, I love whu. \n"
     ]
    }
   ],
   "source": [
    "text= \" i am a college student, i love whu. \"\n",
    "import re\n",
    "pattern = re.compile(r'(?:[^\\w]|\\b)i(?:[^\\w])')\n",
    "while True:\n",
    "    result = pattern.search(text)\n",
    "    if result:\n",
    "        if result.start(0) != 0:\n",
    "            text= text[:result.start(0)+1]+'I'+ text[result.end(0)-1:]\n",
    "        else:\n",
    "            text= text [:result.start(0)]+'I'+ text[result.end(0)-1:]\n",
    "    else:\n",
    "        break\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I love wuhan university.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = ' I love love wuhan university.'\n",
    "pattern = re.compile(r'\\b(\\w+)(\\s+\\1){1,}\\b')\n",
    "matchResult = pattern.search(text)\n",
    "text = pattern.sub(matchResult.group(1), text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Brett\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.945 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 r\n",
      "是 v\n",
      "一名 m\n",
      "大学生 n\n",
      "， x\n",
      "我 r\n",
      "来自 v\n",
      "武汉大学 nt\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "words = pseg.cut(\"我是一名大学生，我来自武汉大学\") \n",
    "for word, flag in words:\n",
    "    print('%s %s' % (word, flag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
